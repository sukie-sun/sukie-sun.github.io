---
title: 大数据思想
tags:
  - 分而治之
categories: 头脑风暴
grammar_cjkRuby: true
description: 关于大数据思维的案例
abbrlink: 17d13c09
date: 2018-12-30 00:00:00
---


### 1、大数据核心问题：

==海量数据、工业技术落后、硬件损坏常态化（Ctrl+M）==

### 2、大数据思维

**分而治之**

> 把一个复杂的问题按一定的“分解”方法分为等价的规模较小的若干部分，然后逐个解决，然后分别找出各部分的中间结果，最后将各个部分的中间结果组成整个问题的最终结果（Ctrl+Q）

![enter description here](https://wx1.sinaimg.cn/large/005zftzDgy1fyw9d7pkpij30pr0eut99.jpg)

### 3、业务场景

仓储、数牌

#### **业务一：找{重复行}(chongfuhang)**

*++现有1TB的TXT文件 ; 
格式：数字+字符 ； 
网速：500M/s ； 
服务器内存大小：128M ；
条件：仅有两行重复 ，内存不能放下全量数据（OM：out of memery）；++*

![enter description here](https://wx1.sinaimg.cn/large/005zftzDgy1fyw9cu9psdj30ht0a1wf1.jpg)

**==方法==**

 答：共需要2次IO：2*30min=1h

==第一次IO==：

 1. 给*`每一行内容加上唯一标记（hashcode（内容），value（行号））`*。
 对每一行内容进行hash运算，得到唯一标识hashcode，作为key，将行号作为value。然而，对于内容完全重复的两行，其hashcode值一定相同。
 2. *`对每一行的hash值进行取模运算，并放置于归类分区的小文件中*`。
    由于数据基数过大，就将每行的key值对取模，转化为小文件，如将hash值对100取模，则产生100个小文件分区，取模后相同的放在同一个分区中。

==第二次IO==：
 3. 在每个分区中的小文件遍历，对每一行进行比较，因为重复行一定会在同一个分区中。这样工作量就会大大减小。

#### **业务二：快{排序}(paixu)**

*++现有1TB的TXT文件 ; 
格式：数字； 
网速：500M/s ； 
服务器内存大小：128M ；
条件：实现快排序，内存不能放下全量数据（OM：out of memery）；++*

两次IO，2 * 30分钟 = 1小时

![enter description here](https://wx1.sinaimg.cn/large/005zftzDgy1fyw9dek3xtj30pr0eu758.jpg)

**==方法一：先全局有序后局部有序==**

 1.对全局按分区排序（由大到小）。
​    用if。。else方法对数据进行按范围分类，落到不同的分区中（0~1000、1001~2000、2001~3000··················）
 2.对局部进行排序（由大到小）。
​    对每个分区进行排序。

![enter description here](https://wx1.sinaimg.cn/large/005zftzDgy1fyw9dj82b6j30pr0ewdgm.jpg)

**==方法二：先局部有序后全局有序==**

 1. 先实现局部有序(小-->大)。
    将文件划分为N个分区，在每个分区内部进行排序
 2. 使用归并实现全局有序。
    每个分区分别各自取出最小值，拿出来比较，最小值放在一旁，最小值所在的那个分区再拿出剩下数据中的最小值，再进行比较，比较出的最小值，排在上一次的最小值后，依次进行下去，这样就实现了全局有序。

![知否](./images/坚持_1.jpg)
