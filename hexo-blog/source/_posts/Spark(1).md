---
title: Sparkå­¦ä¹ ï¼ˆä¸€ï¼‰
tags:
  - è®¡ç®—å¼•æ“
  - åŸºäºå†…å­˜
  - RDDç®—å­
categories: Spark
grammar_cjkRuby: true
mathjax: true
overdue: true
description: 'Sparkçš„è¯¦ç»†å­¦ä¹ ç¬¬ä¸€ç¯‡ç« :Apache Spark æ˜¯ä¸“ä¸ºå¤§è§„æ¨¡æ•°æ®å¤„ç†è€Œè®¾è®¡çš„å¿«é€Ÿé€šç”¨çš„è®¡ç®—å¼•æ“ã€‚'
abbrlink: 84961d20
date: 2019-02-16 21:30:15
update: 2019-02-17 21:30:15
---

---

# ä¸€ã€Sparkç®€ä»‹

## 1ã€ä»€ä¹ˆæ˜¯Sparkï¼Ÿ

> Lightning-fast unified analytics engine
>
> Apache Spark æ˜¯ä¸“ä¸ºå¤§è§„æ¨¡æ•°æ®å¤„ç†è€Œè®¾è®¡çš„å¿«é€Ÿé€šç”¨çš„è®¡ç®—å¼•æ“ã€‚

ç”¨äºé€»è¾‘å›å½’ç®—æ³•ï¼š

å¿«é€Ÿ(100å€)ï¼šèƒ½æ›´å¥½çš„çš„é€‚ç”¨äºæ•°æ®æŒ–æ˜ä¸æœºå™¨å­¦ä¹ ç­‰éœ€è¦è¿­ä»£çš„ç®—æ³•ï¼ˆåœ¨è®¡ç®—ç»“æœçš„åŸºç¡€ä¸Šå†è®¡ç®—ï¼‰ï¼›Jobçš„ä¸­é—´ç»“æœå€¼åœ¨å†…å­˜ä¸­æµè½¬ï¼Œä¸éœ€è¦è¯»å–HDFSï¼Œå±è”½ç£ç›˜å¼€é”€ï¼›DAGè°ƒåº¦

mrï¼šç¦»çº¿ï¼Œï¼ˆè¿­ä»£æ—¶ï¼šç£ç›˜IOï¼Œè¾ƒæ…¢ï¼‰

stormï¼šæµå¼

> Sparkæ˜¯ç”¨Scalaç¼–å†™çš„ï¼Œæ–¹ä¾¿å¿«é€Ÿç¼–ç¨‹


## 2ã€ä¸MapReduceçš„åŒºåˆ«

* MapReduce

<div align="center"><img src="Spark(1)/mr.jpg" align="middle" alt='MapReduceåº•å±‚åŸç†'></div>



<div align="center"><img src="Spark(1)/mr1.jpg" align="middle" alt='MapeduceIOåŸç†'></div>

* Spark

<div align="center"><img src="Spark(1)/spark.jpg" align="middle" alt='SparkIOåŸç†'></div>

åŒºåˆ«ï¼š

> åŒï¼šåˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶
>
> ä¸åŒï¼š
>
> * SparkåŸºäºå†…å­˜ï¼ŒMRåŸºäºHDFS
> * Sparkå¤„ç†æ•°æ®çš„èƒ½åŠ›æ˜¯MRçš„åå€ä»¥ä¸Š
> * Sparké™¤äº†åŸºäºå†…å­˜è®¡ç®—ä¹‹å¤–ï¼Œè¿˜æœ‰DAGæœ‰å‘æ— ç¯å›¾æ¥åˆ‡åˆ†ä»»åŠ¡çš„æ‰§è¡Œé¡ºåº



Spark API  çš„ä½¿ç”¨è¯­è¨€

> Scalaï¼ˆå¾ˆå¥½ï¼‰
> Python(ä¸é”™)
> Java(â€¦)



## 3ã€Sparkè¿è¡Œæ¨¡å¼

* local

å¤šç”¨äºæœ¬åœ°æµ‹è¯•ï¼Œå¦‚åœ¨ eclipseï¼Œidea ä¸­å†™ç¨‹åºæµ‹è¯•

* standalone

standaloneæ˜¯Sparkè‡ªå¸¦çš„èµ„æºè°ƒåº¦æ¡†æ¶ï¼Œå®ƒæ”¯æŒå®Œå…¨åˆ†å¸ƒå¼

* yarn

Hadoopç”Ÿæ€åœˆçš„èµ„æºè°ƒåº¦æ¡†æ¶ï¼ŒSparkä¹Ÿæ˜¯å¯ä»¥åŸºäºyarnæ¥è®¡ç®—çš„

> åŸºäºyarnæ¥è¿›è¡Œèµ„æºè°ƒåº¦ï¼Œå¿…é¡»å®ç°ApplicationMasteræ¥å£ï¼ŒSparkå®ç°çš„è¿™ä¸ªæ¥å£ï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨

* mesos

èµ„æºè°ƒåº¦æ¡†æ¶

# äºŒã€Sparkcore

## 1ã€RDD 

### ï¼ˆ1ï¼‰æ¦‚å¿µï¼š

RDD(Resilient Distributed Dateset)å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†

### ï¼ˆ2ï¼‰äº”å¤§ç‰¹æ€§

> 1. RDD æ˜¯ç”±ä¸€ç³»åˆ—çš„ partition ç»„æˆçš„ã€‚
> 2. å‡½æ•°æ˜¯ä½œç”¨åœ¨æ¯ä¸€ä¸ª partitionï¼ˆsplitï¼‰ä¸Šçš„ã€‚
> 3. RDD ä¹‹é—´æœ‰ä¸€ç³»åˆ—çš„ä¾èµ–å…³ç³»ã€‚
> 4. åˆ†åŒºå™¨æ˜¯ä½œç”¨åœ¨ K,V æ ¼å¼çš„ RDD ä¸Šã€‚
> 5. RDD æä¾›ä¸€ç³»åˆ—æœ€ä½³çš„è®¡ç®—ä½ç½®ã€‚

> * `è·å–RDDçš„æ–¹å¼`
> * parallelize()
>
> > ```java
> > //Distribute a local Scala collection to form an RDD
> > JavaRDD<T> rdd = javaSparkContext.parallelize(List<T> list)ï¼›
> > JavaRDD<T> rdd = javaSparkContext.parallelize(List<T> list,int numSlices)ï¼›    
> >     
> > ```
>
> * parallelizePairs
>
> > ```java
> > //Distribute a local Scala collection to form an RDD
> > JavaPairRDD<K,V> rdd = 
> >      javaSparkContext.parallelizePairs(List<Tuple2<K, V>> list)ï¼›     
> > JavaPairRDD<K,V> rdd = 
> >      javaSparkContext.parallelizePairs(List<Tuple2<K, V>> list,int numSlices)ï¼›
> > ```
> >
> >
>
> * textFile(â€œ./xx.txtâ€)   ä¹Ÿå¯æŒ‡å®šåˆ†åŒº

### ï¼ˆ3ï¼‰RDDç†è§£å›¾

<div align="center"><img src="Spark(1)/rdd1.jpg" align="middle" alt='rdd1åŸç†'></div>



`ç†è®ºæ³¨è§£`

>* RDD  å®é™…ä¸Šä¸å­˜å‚¨æ•°æ®ï¼Œè¿™é‡Œæ–¹ä¾¿ç†è§£ï¼Œæš‚æ—¶ç†è§£ä¸ºå­˜å‚¨æ•°æ®ã€‚
>
>*  textFile æ–¹æ³•åº•å±‚å°è£…çš„æ˜¯MR è¯»å–æ–‡ä»¶çš„æ–¹å¼(å…ˆ split,å†è¯»å–æ–‡ä»¶)ï¼Œé»˜è®¤ split å¤§å°æ˜¯ä¸€ä¸ª block å¤§å°ã€‚
>
>*  RDD æä¾›è®¡ç®—æœ€ä½³ä½ç½®ï¼Œä½“ç°äº†æ•°æ®æœ¬åœ°åŒ–ã€‚ä½“ç°äº†å¤§æ•°æ®ä¸­â€œè®¡ç®—ç§»åŠ¨æ•°æ®ä¸ç§»åŠ¨â€çš„ç†å¿µã€‚
>
>â”  å“ªé‡Œä½“ç° RDD çš„åˆ†å¸ƒå¼ï¼Ÿ
>
>ğŸ‘†  RDD æ˜¯ç”± Partition ç»„æˆï¼Œpartition æ˜¯åˆ†å¸ƒåœ¨ä¸åŒèŠ‚ç‚¹ä¸Šçš„ã€‚
>
>â”  å“ªé‡Œä½“ç° RDD çš„å¼¹æ€§ï¼ˆå®¹é”™ï¼‰ï¼Ÿ
>
> ğŸ‘† partition æ•°é‡ï¼Œå¤§å°æ²¡æœ‰é™åˆ¶,é»˜è®¤å’Œsplitï¼ˆblockï¼‰ä¸€è‡´ï¼Œä½“ç°äº† RDD çš„å¼¹æ€§ã€‚
> ğŸ‘† RDD ä¹‹é—´ä¾èµ–å…³ç³»ï¼Œå¯ä»¥åŸºäºä¸Šä¸€ä¸ª RDD é‡æ–°è®¡ç®—å‡º RDDã€‚
>
>â”  ä»€ä¹ˆæ˜¯ K,V æ ¼å¼çš„ RDD?
>
>ğŸ‘† å¦‚æœ RDD é‡Œé¢å­˜å‚¨çš„æ•°æ®éƒ½æ˜¯äºŒå…ƒç»„å¯¹è±¡ï¼Œé‚£ä¹ˆè¿™ä¸ª RDD æˆ‘ä»¬å°±å«åš K,V æ ¼å¼çš„ RDDã€‚
>
>ğŸ‘† MRæœ‰åˆ†åŒºå™¨ï¼ˆæ ¹æ®keyå€¼æ±‚hashï¼Œæ¥å†³å®šæ•°æ®å­˜æ”¾åœ¨å“ªä¸ªåˆ†åŒºä¸­ï¼Œæ‰€ä»¥åˆ†åŒºå™¨å¿…é¡»ä½œç”¨åœ¨Kï¼ŒVæ ¼å¼çš„RDDä¸Šï¼‰
>
>



<div align="center"><img src="Spark(1)/rdd.jpg" align="middle" alt='rddå…³ç³»'></div>

## 2ã€Sparkä»»åŠ¡æ‰§è¡ŒåŸç†

<div align="center"><img src="Spark(1)/Sparkä»»åŠ¡æ‰§è¡ŒåŸç†.png" align="middle" alt='Sparkä»»åŠ¡æ‰§è¡ŒåŸç†'></div>

<div align="center"><img src="Spark(1)/Sparkä»»åŠ¡æ‰§è¡ŒåŸç†0.png" align="middle" alt='Sparkä»»åŠ¡æ‰§è¡ŒåŸç†'></div>



Driverï¼šï¼ˆç›¸å½“äºApplicationMasterï¼‰

Workerï¼šï¼ˆç›¸å½“äºNodeManagerï¼‰

ä»¥ä¸Šå›¾ä¸­æœ‰å››ä¸ªæœºå™¨èŠ‚ç‚¹ï¼Œ

Driver å’Œ Worker æ˜¯å¯åŠ¨åœ¨èŠ‚ç‚¹ä¸Šçš„è¿›ç¨‹ï¼Œ
è¿è¡Œåœ¨ JVM ä¸­çš„è¿›ç¨‹ã€‚
ïƒ˜ Driver ä¸é›†ç¾¤èŠ‚ç‚¹ä¹‹é—´æœ‰é¢‘ç¹çš„é€šä¿¡ã€‚
ïƒ˜ Driverï¼šä»»åŠ¡çš„è°ƒåº¦ï¼ˆç›‘æ§ä»»åŠ¡ã€ è´Ÿè´£ä»»åŠ¡(tasks)çš„åˆ†å‘å’Œç»“æœçš„å›æ”¶ï¼‰ã€‚å¦‚æœ task
çš„è®¡ç®—ç»“æœéå¸¸å¤§å°±ä¸è¦å›æ”¶äº†ã€‚ä¼šé€ æˆ oomã€‚
ïƒ˜ Worker æ˜¯ Standalone èµ„æºè°ƒåº¦æ¡†æ¶é‡Œé¢èµ„æºç®¡ç†çš„ä»èŠ‚ç‚¹ã€‚ä¹Ÿæ˜¯JVM è¿›ç¨‹ã€‚
ïƒ˜ Master æ˜¯ Standalone èµ„æºè°ƒåº¦æ¡†æ¶é‡Œé¢èµ„æºç®¡ç†çš„ä¸»èŠ‚ç‚¹ã€‚ä¹Ÿæ˜¯JVM è¿›ç¨‹ã€‚

## 3ã€Sparkä»£ç æµç¨‹

### `ä»¥ç”¨Scalaç¼–å†™WordCountä¸ºä¾‹`

1ã€åˆ›å»º SparkConf å¯¹è±¡
ïƒ˜ å¯ä»¥è®¾ç½® Application nameã€‚
ïƒ˜ å¯ä»¥è®¾ç½®è¿è¡Œæ¨¡å¼åŠèµ„æºéœ€æ±‚ã€‚

```scala
 val conf = new SparkConf()
        /**
          * å‡ ç§è¿è¡Œæ–¹å¼ï¼š
          *   1.æœ¬åœ°è¿è¡Œ
          *   2.yarn
          *   3.standalone
          *   4.mesos
          */
  conf.setMaster("local").setAppName("wc")
```

2ã€åˆ›å»º SparkContext å¯¹è±¡

```scala
 val  context = new SparkContext(conf)
```

3ã€åŸºäº Spark çš„ä¸Šä¸‹æ–‡åˆ›å»ºä¸€ä¸ª RDDï¼Œå¯¹ RDD è¿›è¡Œå¤„ç†ã€‚

```scala
//è·å–æ–‡ä»¶ä¸­æ¯ä¸€è¡Œæ•°æ®çš„ADD
 val lineADD = context.textFile("./wc.txt")
//è·å–æ¯ä¸€è¡Œæ•°æ®æŒ‰ç©ºæ ¼åˆ‡åˆ†åçš„ADD
 val wordADD = lineADD.flatMap(x=>{x.split(" ")})
//è·å–æ¯ä¸ªå•è¯åŠ ä¸Š,1 åçš„ADDï¼ˆK,Væ ¼å¼ï¼‰
 val KVADD = wordADD.map(x=>{(x,1)})
//è·å–å°†ç›¸åŒkeyçš„valueç›¸åŠ åçš„ADDï¼ˆK,Væ ¼å¼ï¼‰ï¼Œç›¸å½“äºTuple2
 val resultADD = KVADD.reduceByKey((x,y)=>{x+y})
//é™åºæ’åº
 val sortADD = resultADD.sortBy(_._2,false)
```

4ã€åº”ç”¨ç¨‹åºä¸­è¦æœ‰ Action ç±»ç®—å­æ¥è§¦å‘ Transformation ç±»ç®—å­æ‰§è¡Œã€‚

```scala
sortADD.foreach(println)
```

5ã€å…³é—­ Spark ä¸Šä¸‹æ–‡å¯¹è±¡ SparkContextã€‚

```scala
context.stop()
```



## 4ã€Transformations è½¬æ¢ç®—å­

### ï¼ˆ1ï¼‰æ¦‚å¿µ

Transformations ç±»ç®—å­æ˜¯ä¸€ç±»ç®—å­ï¼ˆå‡½æ•°ï¼‰å«åšè½¬æ¢ç®—å­ï¼Œå¦‚map,flatMap,reduceByKey ç­‰ã€‚Transformations ç®—å­æ˜¯å»¶è¿Ÿæ‰§è¡Œï¼Œä¹Ÿå«æ‡’åŠ è½½æ‰§è¡Œã€‚

> æœ‰actionè§¦å‘ç®—å­ä»»åŠ¡æ‰èƒ½æäº¤ï¼Œæ‰ä¼šæ‰§è¡Œrunjob
>
> ç®—å­å¿…é¡»ä½œç”¨åœ¨RDDä¸Š

### ï¼ˆ2ï¼‰Transformation ç±»ç®—å­

> :arrow_up_small: **filter**
> è¿‡æ»¤ç¬¦åˆæ¡ä»¶çš„è®°å½•æ•°ï¼Œtrue ä¿ç•™ï¼Œfalse è¿‡æ»¤æ‰ã€‚
>
> ğŸ”¼ **contains**
>
> ä½œä¸ºæ¡ä»¶ï¼Œæ˜¯å¦åŒ…å«ï¼Œè¿”å›true|false



> :arrow_up_small:**map**
> å°†ä¸€ä¸ª RDD ä¸­çš„æ¯ä¸ªæ•°æ®é¡¹ï¼Œé€šè¿‡ map ä¸­çš„å‡½æ•°æ˜ å°„å˜ä¸ºä¸€ä¸ªæ–°çš„å…ƒç´ ã€‚
> ç‰¹ç‚¹ï¼šè¾“å…¥ä¸€æ¡ï¼Œè¾“å‡ºä¸€æ¡æ•°æ®ã€‚
>
> :black_joker: **mapToPair**   (Java)
>
> å°†RDDï¼ˆå¦‚lineRDDï¼‰è½¬æ¢æˆäºŒå…ƒç»„
>
> ğŸƒ **mapValues**
>
> æ“ä½œï¼ˆK,Vï¼‰RDDä¸­çš„value     è¿”å›Tuple2<>
>
> :arrow_up_small: **flatMap**
> å…ˆ map å flatã€‚ä¸ map ç±»ä¼¼ï¼Œæ¯ä¸ªè¾“å…¥é¡¹å¯ä»¥æ˜ å°„ä¸º 0 åˆ°å¤šä¸ªè¾“å‡ºé¡¹ã€‚
>
> ğŸ”¼ **mapPartition**
>
> ä¸ map ç±»ä¼¼ï¼Œéå†çš„å•ä½æ˜¯æ¯ä¸ª partition ä¸Šçš„æ•°æ®ã€‚ä¸€è¿›ä¸€å‡º
>
> ğŸ”¼**mapPartitionWithIndex**
> ç±»ä¼¼äº mapPartitions,é™¤æ­¤ä¹‹å¤–è¿˜ä¼šæºå¸¦åˆ†åŒºçš„ç´¢å¼•å€¼ã€‚
>
> ğŸ”¼ **repartition**
>
> repartitionï¼ˆ3ï¼‰
>
> å¢åŠ æˆ–å‡å°‘åˆ†åŒº.ä¼šäº§ç”Ÿshuffle
>
> ğŸ”¼**coalesce**
>
> coalesce(3,false)
>
> å¸¸ç”¨äºå‡å°‘åˆ†åŒºï¼Œç¬¬äºŒä¸ªå‚æ•°å†³å®šå‡å°‘åˆ†åŒºæ—¶æ˜¯å¦äº§ç”Ÿshuffleï¼štrue ä¸ºäº§ç”Ÿ shuffleï¼Œfalse ä¸äº§ç”Ÿ shuffleã€‚é»˜è®¤æ˜¯ `false`ã€‚
>
> å¦‚æœ coalesce è®¾ç½®çš„åˆ†åŒºæ•°æ¯”åŸæ¥çš„ RDD çš„åˆ†åŒºæ•°è¿˜å¤šçš„è¯ï¼Œç¬¬äºŒä¸ªå‚æ•°è®¾ç½®ä¸º `false` ä¸ä¼šèµ·ä½œç”¨ï¼Œ
>
> å¦‚æœè®¾ç½®æˆ trueï¼Œæ•ˆæœå’Œ repartition ä¸€æ ·ã€‚å³ 
>
> repartition(numPartitions) = coalesce(numPartitions,true)



> :arrow_up_small:**sample**
> éšæœºæŠ½æ ·ç®—å­ï¼Œæ ¹æ®ä¼ è¿›å»çš„å°æ•°æŒ‰æ¯”ä¾‹è¿›è¡Œï¼Œæœ‰æ”¾å›æˆ–è€…æ— æ”¾å›çš„æŠ½æ ·ã€‚
>
> :arrow_up_small:**reduceByKey**
> å¯¹äºKï¼ŒVæ ¼å¼çš„RDDï¼Œå°†keyç›¸åŒçš„RDDï¼Œå¯¹å…¶valueå€¼æ ¹æ®ç›¸åº”çš„é€»è¾‘è¿›è¡Œå¤„ç†ã€‚
>
> ğŸ”¼**reduceByKeyAndWindow** (f1,f2,s1,s2)  
>
> çª—å£å‡½æ•°
>
> :arrow_up_small:**sortByKey/sortBy**
> ä½œç”¨åœ¨ K,V æ ¼å¼çš„ RDD ä¸Šï¼Œå¯¹ key è¿›è¡Œå‡åºæˆ–è€…é™åºæ’åºã€‚



> :arrow_up_small:**join / leftOuterJoin / rightOuterJoin / fullOuterJoin**
>
> join ï¼šä¿ç•™å…¬å…±å…ƒç´  ï¼ˆK,Vï¼‰
>
> leftOutJoin ï¼šä¿ç•™å·¦è¾¹çš„å…ƒç´ 
>
> rightOutJoin ï¼šä¿ç•™å³è¾¹å…ƒç´ 
>
> fullOutJoin ï¼šå»é‡ä¿ç•™ ï¼ˆä¿ç•™æœ€å¤§åˆ†åŒºæ•°ï¼‰
>
> ä½œç”¨åœ¨ K,V æ ¼å¼çš„ RDD ä¸Šã€‚æ ¹æ® K è¿›è¡Œè¿æ¥ï¼Œå¯¹ï¼ˆK,Vï¼‰join(K,W)è¿”å›ï¼ˆK,(V,W)ï¼‰
>
> * join åçš„åˆ†åŒºæ•°ä¸çˆ¶ RDD åˆ†åŒºæ•°å¤šçš„é‚£ä¸€ä¸ªç›¸åŒ 
>
> ğŸ”¼**union**
>
> éƒ½ä¿ç•™ ï¼ˆä¿ç•™æ€»åˆ†åŒºæ•°ï¼‰
>
> åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†ã€‚ä¸¤ä¸ªæ•°æ®é›†çš„ç±»å‹è¦ä¸€è‡´ã€‚
>
> * è¿”å›æ–°çš„ RDD çš„åˆ†åŒºæ•°æ˜¯åˆå¹¶ RDD åˆ†åŒºæ•°çš„æ€»å’Œã€‚
>
> ğŸ”¼**intersection**
>
> å–ä¸¤ä¸ªæ•°æ®é›†çš„äº¤é›†
>
> ğŸ”¼ **subtract**
>
> å–ä¸¤ä¸ªæ•°æ®é›†çš„å·®é›†
>
> ğŸ”¼ **distinct**(map+reduceByKey+map)
>
> å»é‡



> ğŸ”¼ **cogroup**
>
> å½“è°ƒç”¨ç±»å‹ï¼ˆK,Vï¼‰å’Œï¼ˆKï¼ŒWï¼‰çš„æ•°æ®ä¸Šæ—¶ï¼Œè¿”å›ä¸€ä¸ªæ•°æ®é›†ï¼ˆKï¼Œï¼ˆIterable<V>,Iterable<W>ï¼‰ï¼‰
>
> ğŸ”¼**groupByKey**
> ä½œç”¨åœ¨ Kï¼ŒV æ ¼å¼çš„ RDD ä¸Šã€‚æ ¹æ® Key è¿›è¡Œåˆ†ç»„ã€‚è¿”å›ï¼ˆKï¼ŒIterable <V>ï¼‰ã€‚



> ğŸ”¼**zip**
> å°†ä¸¤ä¸ª RDD ä¸­çš„å…ƒç´ ï¼ˆKV æ ¼å¼/é KV æ ¼å¼ï¼‰å˜æˆä¸€ä¸ª KV æ ¼å¼çš„ RDD,ä¸¤ä¸ª RDD çš„ä¸ªæ•°å¿…é¡»ç›¸åŒã€‚
>
> ğŸ”¼**zipWithIndex**
> è¯¥å‡½æ•°å°† RDD ä¸­çš„å…ƒç´ å’Œè¿™ä¸ªå…ƒç´ åœ¨ RDD ä¸­çš„ç´¢å¼•å·ï¼ˆä» 0 å¼€å§‹ï¼‰ç»„åˆæˆï¼ˆK,Vï¼‰å¯¹ã€‚
>
> ğŸ”¼



```scala
object WordCount {
  def main(args: Array[String]): Unit = {
      
    val conf = new SparkConf()
      
    conf.setMaster("local").setAppName("WC")
      
    val context = new SparkContext(conf)  //ç”¨äºäº†è§£é›†ç¾¤
      
    val linesRDD :RDD[String] = context.textFile("./words.txt")
      
//  lineRDD.filter(x=>{
//            x.contains("sh")
//        }).foreach(println)

//  lineRDD.sample(true,0.2).foreach(println)
     
//  lineRDD.map((_,1)).reduceByKey(_ + _).sortBy(_._2,false).foreach(println)

//  lineRDD.map((_,1)).sortByKey().foreach(println)
         
    val wordRDD :RDD[String]  = linesRDD.flatMap{lines => {
      lines.split(" ")  //åŒ¿åå‡½æ•°
    }}
      
    val KVRDD:RDD[(String,Int)] = wordRDD.map{ x => (x,1) }
 
      
    val result:RDD[(String,Int)] = KVRDD.reduceByKey{(a,b)=> {
        println("a:"+a+",b:"+b)
        a+b    
    }}
```

`è¡¥å……`

```java
    val conf = new SparkConf()
      
    conf.setMaster("local").setAppName("WC")
      
    val context = new SparkContext(conf)  //ç”¨äºäº†è§£é›†ç¾¤
      
   //parallelizePairs

   //join
Optional.absent(0)
optional.isPresent()
optinal.get()
```



## 5ã€Action è¡ŒåŠ¨ç®—å­

### ï¼ˆ1ï¼‰æ¦‚å¿µ

Action ç±»ç®—å­ä¹Ÿæ˜¯ä¸€ç±»ç®—å­ï¼ˆå‡½æ•°ï¼‰å«åšè¡ŒåŠ¨ç®—å­ï¼Œå¦‚foreach,collectï¼Œcount ç­‰ã€‚

Transformations ç±»ç®—å­æ˜¯å»¶è¿Ÿæ‰§è¡Œï¼ŒAction ç±»ç®—å­æ˜¯è§¦å‘æ‰§è¡Œï¼ˆç«‹å³ï¼‰ã€‚

> ä¸€ä¸ª application åº”ç”¨ç¨‹åºä¸­æœ‰å‡ ä¸ª Action ç±»ç®—å­æ‰§è¡Œï¼Œå°±æœ‰å‡ ä¸ª job è¿è¡Œã€‚

### ï¼ˆ2ï¼‰Action ç±»ç®—å­

> :arrow_up_small: **count**
> è¿”å›æ•°æ®é›†ä¸­çš„å…ƒç´ æ•°ã€‚ä¼šåœ¨ç»“æœè®¡ç®—å®Œæˆåå›æ”¶åˆ° Driver ç«¯ã€‚
>
> ğŸ”¼**countByKey**
> ä½œç”¨åˆ° K,V æ ¼å¼çš„ RDD ä¸Šï¼Œæ ¹æ® Key è®¡æ•°ç›¸åŒ Key çš„æ•°æ®é›†å…ƒç´ ã€‚
>
> ğŸ”¼**countByValue**
> æ ¹æ®æ•°æ®é›†æ¯ä¸ªå…ƒç´ ç›¸åŒçš„å†…å®¹æ¥è®¡æ•°ã€‚è¿”å›ç›¸åŒå†…å®¹çš„å…ƒç´ å¯¹åº”çš„æ¡æ•°ã€‚
>
>
>
> :arrow_up_small: **take(n)**
> è¿”å›ä¸€ä¸ªåŒ…å«æ•°æ®é›†å‰ n ä¸ªå…ƒç´ çš„é›†åˆã€‚
> :arrow_up_small: **first**
> first=take(1),è¿”å›æ•°æ®é›†ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
>
> ğŸ”¼ **collect**
> å°†è®¡ç®—ç»“æœå›æ”¶åˆ° Driver ç«¯ã€‚
>
> :arrow_up_small: **foreach**
> å¾ªç¯éå†æ•°æ®é›†ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼Œè¿è¡Œç›¸åº”çš„é€»è¾‘ã€‚
>
> :arrow_up_small: **foreachPartition**
>
> éå†çš„æ•°æ®æ˜¯æ¯ä¸ª partition çš„æ•°æ®ã€‚æ‰€ä»¥ä¼ çš„å‚æ•°ä¸ºIterator
>
> :arrow_up_small:**reduce**
> æ ¹æ®èšåˆé€»è¾‘èšåˆæ•°æ®é›†ä¸­çš„æ¯ä¸ªå…ƒç´ ã€‚

```scala
    val conf = new SparkConf()
    conf.setMaster("local").setAppName("transf")

    val context = new SparkContext(conf)

    val lineADD = context.textFile("./wc.txt")
    val wordADD = lineADD.flatMap(x=>{x.split(" ")})

   // println(wordADD.count())

//lineADDä¸­æ•°æ®å›æ”¶
    val arr= lineADD.collect()
    arr.foreach(println)

//  val takes: Array[String] = lineRDD.take(5)
//  takes.foreach(println)

//  val str: String = lineRDD.first()
//  println(str)
```

## 6ã€æ§åˆ¶ç®—å­

### ï¼ˆ1ï¼‰æ¦‚å¿µï¼š

* æ§åˆ¶ç®—å­æœ‰ä¸‰ç§ï¼Œcacheã€persistã€checkpoint
* ä»¥ä¸Šç®—å­éƒ½å¯ä»¥å°†RDD æŒä¹…åŒ–ï¼ŒæŒä¹…åŒ–çš„å•ä½æ˜¯ partitionã€‚
* cache å’Œ persist éƒ½æ˜¯æ‡’ æ‰§è¡Œçš„ã€‚
* å¿…é¡»æœ‰ä¸€ä¸ª action ç±»ç®—å­è§¦å‘æ‰§è¡Œã€‚
* cache å’Œ persist ç®—å­çš„è¿”å›å€¼å¯èµ‹å€¼ç»™ä¸€ä¸ªå˜é‡ï¼Œåœ¨å…¶ä»– job ä¸­ç›´æ¥ä½¿ç”¨è¿™ä¸ªå˜é‡å°±æ˜¯ä½¿ç”¨æŒä¹…åŒ–çš„æ•°æ®äº†
* checkpoint ç®—å­ä¸ä»…èƒ½å°† RDD æŒä¹…åŒ–åˆ°ç£ç›˜ï¼Œè¿˜èƒ½åˆ‡æ–­ RDD ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼ˆæ‰€æœ‰çˆ¶RDDï¼‰ã€‚
* `é”™è¯¯ï¼š`rdd.cache().count() è¿”å›çš„ä¸æ˜¯æŒä¹…åŒ–çš„ RDDï¼Œè€Œæ˜¯ä¸€ä¸ªæ•°å€¼äº†ã€‚

### ï¼ˆ2ï¼‰è¯¦è§£

> 1ï¸âƒ£**â€‹ cache**
> é»˜è®¤å°† RDD çš„æ•°æ®æŒä¹…åŒ–åˆ°å†…å­˜ä¸­ã€‚cache æ˜¯æ‡’æ‰§è¡Œã€‚
>
> * `æ³¨æ„`ï¼š
>
> chche () =persist()=persist(StorageLevel.Memory_Only)

> 2ï¸âƒ£ **persist** 
>
> æ”¯æŒæŒ‡å®šæŒä¹…åŒ–çº§åˆ«
>
> useOffHeap  ä½¿ç”¨å †å¤–å†…å­˜
>
> diskã€memoryã€offheapã€deserializedï¼ˆä¸åºåˆ—åŒ–ï¼‰ã€replicationï¼ˆå‰¯æœ¬æ•°ï¼Œé»˜è®¤ä¸º1ï¼‰
>
> åºåˆ—åŒ–ï¼šå‹ç¼©æ•°æ®ï¼ˆèŠ‚çœç©ºé—´ï¼Œä½¿ç”¨æ•°æ®æ—¶è¦ååºåˆ—åŒ–ï¼Œä¼šé¢å¤–æ¶ˆè€—CPUæ€§èƒ½ï¼‰
>
> none ã€disk_onlyã€disk_only_2ã€memeory_only ã€memeory_only _ser ã€ memory_and_disk ã€ memory_and_disk_2

> 3ï¸âƒ£ **checkpoint**  
>
> checkpoint å°† RDD æŒä¹…åŒ–åˆ°ç£ç›˜ï¼Œè¿˜å¯ä»¥åˆ‡æ–­ RDD ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚
>
> * checkpoint çš„æ‰§è¡ŒåŸç†ï¼š
>
> 1. å½“ RDD çš„ job æ‰§è¡Œå®Œæ¯•åï¼Œä¼šä» finalRDD ä»åå¾€å‰å›æº¯ã€‚
> 2. å½“å›æº¯åˆ°æŸä¸€ä¸ª RDD è°ƒç”¨äº† checkpoint æ–¹æ³•ï¼Œä¼šå¯¹å½“å‰çš„RDD åšä¸€ä¸ªæ ‡è®°ã€‚
> 3. Spark æ¡†æ¶ä¼šè‡ªåŠ¨å¯åŠ¨ä¸€ä¸ªæ–°çš„ jobï¼Œé‡æ–°è®¡ç®—è¿™ä¸ª RDD çš„æ•°æ®ï¼Œå°†æ•°æ®æŒä¹…åŒ–åˆ° HDFS ä¸Šã€‚
>
> * ä¼˜åŒ–ï¼š
>
> å¯¹ RDD æ‰§è¡Œ checkpoint ä¹‹å‰ï¼Œæœ€å¥½å¯¹è¿™ä¸ª RDD å…ˆæ‰§è¡Œcacheï¼Œè¿™æ ·æ–°å¯åŠ¨çš„ job åªéœ€è¦å°†å†…å­˜ä¸­çš„æ•°æ®æ‹·è´åˆ° HDFSä¸Šå°±å¯ä»¥ï¼Œçœå»äº†é‡æ–°è®¡ç®—è¿™ä¸€æ­¥ã€‚

æŒä¹…åŒ–çº§åˆ«ï¼šå¦‚ä¸‹

![](https://wx1.sinaimg.cn/large/005zftzDgy1g098pwwdc9j30f70aztbu.jpg)

```scala
val cocnf = new SparkConf()
conf.setMaster("local").setAppname("count")
val context = new SparkContext()

//è®¾ç½®CPåœ¨HDFSä¸Šçš„è·¯å¾„
context.setCheckPointDir("")

val lineADD = context.textFile("./countword.txt")
val time1 = System.currentTimeMillis()
val c =  lineADD.count()
val time2 = System.currentTimeMillis()
val t1 = time2 - time1

//åšç¼“å­˜(persisitï¼ˆm_oï¼‰)
linelineADD = lineADD.cache()
//åšæŒä¹…åŒ–
lineADD.persisit(StorageLevel.memory_only)
//checkpoint å®¹é”™,æœ€å¥½è¿˜æœ‰cache
lineADD.checkpoint()

val time3 = System.currentTimeMillis()
val c =  lineADD.count()
val time4 = System.currentTimeMillis()
val t2 = time4 - time3

//t1 è¿œå¤§äº t2
```



## WordCount

### `ä»¥ç”¨Javaç¼–å†™ä¸ºä¾‹`

```java
package com.shsxt.spark.java;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.*;
import scala.Tuple2;

import java.util.Arrays;
import java.util.List;

public class WordCount {

    public static void main(String[] args) {


        SparkConf conf = new SparkConf();

        conf.setMaster("local").setAppName("wc");

        JavaSparkContext context = new JavaSparkContext(conf);

        JavaRDD<String> rdd = context.textFile("./wc.txt");


        long count = rdd.count();

        List<String> collect = rdd.collect();

        List<String> take = rdd.take(5);

        String first = rdd.first();

        JavaRDD<String> wordRDD = rdd.flatMap(new FlatMapFunction<String, String>() {
            @Override
            public Iterable<String> call(String line) throws Exception {

                String[] split = line.split(" ");

                List<String> list = Arrays.asList(split);

                return list;
            }
        });

//        wordRDD.map(new Function<String>() {
//            @Override
//            public String call(String v1) throws Exception {
//                return null;
//            }
//        })


        JavaPairRDD<String, Integer> pairRDD = wordRDD.mapToPair(new PairFunction<String, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(String word) throws Exception {
                return new Tuple2(word, 1);
            }
        });


        JavaPairRDD<String, Integer> resultRDD = pairRDD.reduceByKey(new Function2<Integer, Integer, Integer>() {
            @Override
            public Integer call(Integer v1, Integer v2) throws Exception {
                return v1 + v2;
            }
        });


        JavaPairRDD<Integer, String> reverseRDD = resultRDD.mapToPair(new PairFunction<Tuple2<String, Integer>, Integer, String>() {
            @Override
            public Tuple2<Integer, String> call(Tuple2<String, Integer> tuple2) throws Exception {

                return new Tuple2<>(tuple2._2, tuple2._1);
            }
        });

        JavaPairRDD<Integer, String> sortByKey = reverseRDD.sortByKey(false);

        JavaPairRDD<String, Integer> result = sortByKey.mapToPair(new PairFunction<Tuple2<Integer, String>, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(Tuple2<Integer, String> tuple2) throws Exception {
                return new Tuple2<>(tuple2._2, tuple2._1);

            }
        });

        result.foreach(new VoidFunction<Tuple2<String, Integer>>() {
            @Override
            public void call(Tuple2<String, Integer> tuple2) throws Exception {
                System.out.println(tuple2);
            }
        });

    }
}

```



Linxç³»ç»Ÿå®šæ—¶è°ƒåº¦ï¼š

crontab

å®šæ—¶è°ƒåº¦è„šæœ¬æ–‡ä»¶

è„šæœ¬æ–‡ä»¶ä¸­ï¼Œç¼–è¾‘spark æäº¤å‘½ä»¤

>  `æ³¨æ„ï¼š`  è„šæœ¬æ–‡ä»¶ä¸­çš„å‘½ä»¤å¿…é¡»å†™å®ƒçš„å®Œæ•´è·¯å¾„ï¼Œå¦åˆ™æ‰¾ä¸åˆ°æ­¤å‘½ä»¤















